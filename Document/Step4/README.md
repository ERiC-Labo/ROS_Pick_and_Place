# 画像処理ノードの作成
# どうすれば把持位置を求められるか
人間は、高性能なセンサーを持っており(目、耳、鼻、皮膚)、センサーで収集した情報を高性能なコンピュータ(脳)で処理し、コンピュータからの司令をマニピュレータ(腕、足)に伝達し、物体を把持することができます。ロボットも同様にセンサからの情報をコンピュータで処理し、マニピュレータを動かすことで目的を達成することができます。<br>
今回用いるセンサはRGBカメラで、このRGBカメラで撮影された画像をもとに把持位置を決定します。それではカメラからの画像を見てみましょう。<br>
![Screenshot from 2022-06-18 22-29-15](https://user-images.githubusercontent.com/75206988/174439828-7b50c757-5fad-4166-855d-21cb2b83cbe7.png)
この画像から対象物体を見つけ、どこを目標にしてつかみに行けばいいかは我々人間であれば簡単にわかりますが、コンピュータには難しい課題です。そのため、コンピュータでも処理できるようにするために人間が問題を簡単にしてあげる必要があります。<br>
まずは、対象物体だけを強調した画像を作りましょう。対象物体だけを強調した画像を作る際にまず考えることは、対象物体だけが持つ特徴について考えることです。この画像の場合では、対象物体以外に赤い物体は存在しません。今回は、「対象物体は赤色である」という特徴に注目します。画像処理ノードでは、カメラからの画像をHSV画像に変換して赤色の画像の領域を抽出し、対象部分の領域を切り取ったマスク画像を作成しました。作成したマスク画像は以下のようになります。<br>
![Screenshot from 2022-06-18 22-48-07](https://user-images.githubusercontent.com/75206988/174441380-b59bb319-c8ee-4e97-9ce8-c1d17620be28.png)
対象領域を抽出したマスク画像から把持位置を求めましょう。把持位置となる対象物体を代表する点はいろいろありますが、今回は重心座標を把持位置とします。
# ピクセル座標系からワールド座標系への変換
先程までの処理で、把持位置を求めることができました。しかしながら、求めた把持位置の座標はピクセル値なのでロボットを動かすには、メートル単位のワールド座標系に変換してあげる必要があります。ピクセル座標系からワールド座標系へ変換する最も簡単な方法はピンホールカメラモデルを考えることです。下の画像をご覧ください。<br>
![image](https://user-images.githubusercontent.com/75206988/174442053-f81a8ed7-04ae-4381-9a9b-e78bdf99a9c5.png)
R(X, Y)をワールド座標系の特徴点、Q(u, v)をピクセル座標系の特徴点、P(Cx, Cy)を画像中心、f(fx, fy)をカメラの焦点距離とすると、画像中の三角形OPQと三角形OSRの相似関係を利用すればピクセル座標系からワールド座標系に変換することができます。<br>
具体的には、特徴点までの距離をZとすると、<br>
X = Z(u-Cx)/fx<br>
とすると求めることができます。<br>
特徴点までの距離は、通常であればステレオカメラなどを用いて求めますが、今回は既知とします。
# ROSでの通信処理
画像処理ノードでのROSが行う処理について説明します。<br>
シュミレーター内のカメラから"/camera/image_raw"というトピックで画像が配信されているので画像を受け取り、画像に対して処理を行います。ROSのメッセージ形式の画像はOpenCVで扱うことができないので、まずはCvBridgeを利用してメッセージからOpenCVで扱える形式に変換します。その後画像処理を施し、作成したマスク画像を"/mask_img"というトピックで配信します。また、求めた把持位置を"/picking_position"というトピックで配信します。
# 画像処理ノードの実行方法
- cd ~/Catkin_ws
- source devel/setup.bash
- rosrun pick_and_place image_processing.py
注意：画像処理ノードを実行できない場合は以下の処理を行ってください。
- cd ~/Catkin_ws/src/ROS_Pick_and_Place/pick_and_place/scripts
- chmod +x image_processing.py 
